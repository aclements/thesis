\section{Achieving conflict-freedom in POSIX}

Given that many commutative operations are not conflict-free in Linux,
is it feasible to build file system and virtual memory systems that
do achieve conflict-freedom?
%
To answer this
question, we designed and implemented a \code{ramfs}-like in-memory
file system called \fs and a virtual memory system called RadixVM for
\sys, our research kernel based on xv6~\cite{xv6}.
%
Although it is in principle possible to make the same changes in Linux,
we chose not to implement \fs in Linux because \fs's design
would have required extensive changes throughout the Linux kernel.
%
The designs of both RadixVM and \fs were guided by the
commutativity rule.  For \fs, we relied heavily on \tool throughout
development to guide its design and identify sharing problems in its
implementation.  RadixVM
was built prior to
\tool, but was guided by manual reasoning about commutativity and
conflicts, which we later validated using \tool.

\begin{figure*}
\small
\centering
\inputnodraft{figures/testcases-sv6}
\caption{Conflict-freedom of commutative system call pairs in \sys.}
\label{fig:testcase-breakdown-sv6}
\end{figure*}

\Cref{fig:testcase-breakdown-sv6} shows the result of applying \tool
to \sys.  In contrast with Linux, \sys is conflict-free for nearly
every commutative test case.  The rest of this chapter explores how we
accomplish this in \sys to answer the following questions:

\begin{CompactItemize}

\item What techniques are necessary to achieve scalability for
      cases where current file and virtual memory systems do not
      scale?

\item What situations might be too difficult or impractical to
      make scale, despite being commutative?

\end{CompactItemize}

This chapter starts with the design of Refcache, a scalable reference
counting mechanism used throughout \sys.  It then covers the designs
of RadixVM and \fs.  Finally, it discusses operations that are
difficult to make conflict-free without sacrificing other practical
concerns.

\input{sv6-refcache}

\input{sv6-radixvm}

\subsection{\fs: Conflict-free file system operations}

\XXX![AC]{This section may end up with redundant parts.  It may need
  more details about the design, or at least an intro that more
  strongly states that, unlike Refcache and RadixVM, \fs is all known
  techniques and \tool-guided elbow grease.}

\fs makes extensive use of existing techniques for scalable
implementations, such as per-core resource
allocation, double-checked locking, lock-free readers using
RCU~\cite{rcu:linux},
scalable reference counts using Refcache~\cite{clements:radixvm},
and seqlocks~\cite[\S6]{lameter:linuxsync}.  These techniques lead to
several common patterns, as follows; we illustrate the patterns with
example test cases from \tool{} that led us to discover these situations:


\paragraph{Layer scalability.}  \fs uses data structures that
themselves naturally satisfy the commutativity rule, such as linear
arrays, radix arrays~\cite{clements:radixvm}, and hash tables.  In
contrast with structures like balanced trees, these data
structures
typically share no cache lines when different elements are accessed
or modified.  For example, \fs stores the cached data pages for a given inode
using a radix array, so that concurrent reads or writes to different
file pages scale, even in the presence of operations
extending or truncating the file.
% , as long as they don't access the part
% of the file that's being extended or truncated.
Many operations also use this radix array to determine if some offset
is within the file's bounds without risking conflicts with operations
that change the file's size.

\XXX[AC]{Nir's book calls such data structures ``naturally parallel''
or something.}


\paragraph{Defer work.} Many kernel resources are shared,
such as files and pages, and must be freed when no longer referenced.
Typically, kernels release resources immediately, but this requires
eagerly tracking references to resources, causing
commutative operations that access the same resource to conflict.  Where
releasing a resource is not time-sensitive, \fs
uses Refcache~\cite{clements:radixvm} to batch reference count
reconciliation and zero detection.  This way, resources are eventually
released, but within each Refcache epoch commutative operations can be
conflict-free.

\XXX[AC]{Discuss crazy pipe hybrid reference counting scheme?}

\XXX[AC]{This seems uninteresting, and possibly false. \par
Batching sometimes enables absorption, when a resource gets reused within
a batch interval, thus requiring no non-scalable operations at all.
For instance, if one process closes the last open file descriptor for
a file, that file becomes eligible for eviction from the inode cache
(using Refcache weak references~\cite{clements:radixvm}).  However,
if another process re-opens the file on the same core, the inode can
be revived without any conflicting memory accesses.}

Some resources are artificially scarce, such as inode numbers in a typical
Unix file system.  When a typical Unix file system runs out of free
inodes, it must reuse an inode from a recently deleted file.  However,
the POSIX interface does not require that inode numbers be reused, only
that the same inode number is not used for two files at once.  Thus,
\fs never reuses inode numbers.  Instead, inode numbers are
generated by a monotonically increasing per-core counter, concatenated
with the core number that allocated the inode.  This allows \fs to defer
inode garbage collection for longer periods of time, and enables scalable
per-core inode allocation.


% \paragraph{Check before updating.}  Before updating any data structure,
% \fs first checks whether the data structure already contains the new
% value, and avoid any writes if so.  For example, the Linux file system
% always acquires a lock on an inode when truncating the file.  This means
% that concurrent truncate operations do not scale, even if the file
% is already empty.  \fs first checks the current length of the file,
% and avoids updating the length or acquiring any locks if the file is
% already at the right size.

% As another example, if one thread tries to create an existing file using
% \code{open(O_CREAT|O_EXCL)}, it should fail, but a na\"ive implementation
% might first acquire a lock to prepare for creating the file, and then
% check whether the file already exists.  This makes
% concurrent \code{open(O_CREAT|O_EXCL)} calls for an existing file name
% non-scalable.  \fs first checks whether the file already exists without
% modifying any cache lines, making these operations scale.

% \XXX[AC]{The above two examples are slightly off.  E.g., if I do two
% truncates and the first writes to the file length and the second reads
% that, that's sharing.  These are both idempotent updates, which is one
% of the few things we \emph{don't} do scalably.}

% As a final example, if two file names \code{a} and \code{b} point to the
% same inode, \code{rename(a, b)} should remove the directory entry for
% \code{a}, but it should not modify the directory entry for \code{b}, since
% it already points at the right inode.  By checking the directory
% entry for \code{b} before updating it, \fs allows \code{rename(a, b)}
% to scale with other operations that look up \code{b}.


\paragraph{Precede pessimism with optimism.} Many operations
in \fs have an optimistic check stage followed by a pessimistic update
stage, a generalized sort of double-checked locking.  The optimistic
stage checks conditions for the operation and returns immediately if
no updates are necessary (this is often the case for error returns,
but can also happen for success returns).  This stage does no writes
or locking, but because no updates are necessary, it is often easy to
make atomic.  If updates are necessary, the operation acquires
locks or uses lock-free protocols, re-verifies its conditions to
ensure atomicity of the update stage, and performs updates.  For
example, \code{lseek} first computes the new offset using a lock-free
read-only protocol and returns early if the new offset is invalid or
equal to the current offset.  Otherwise, \code{lseek} locks the file
offset, and re-computes the new offset to ensure consistency.

\code{rename} is similar.  If two file names \code{a} and \code{b}
point to the
same inode, \code{rename(a, b)} should remove the directory entry for
\code{a}, but it does not need to modify the directory entry for
\code{b}, since
it already points at the right inode.  By checking the directory
entry for \code{b} before updating it, \code{rename(a, b)} avoids
conflicts with other operations that look up \code{b}.


\paragraph{Don't read unless necessary.}  A common internal interface
in a file system implementation is a \code{namei} function that
checks whether a path name exists, and if so, returns the inode for
that path.
%
However, reading the inode is unnecessary
if the caller wants to know only whether a path name existed, such as
an \code{access(F_OK)} system call.  In particular, the \code{namei}
interface makes it impossible for concurrent \code{access(b, F_OK)}
and \code{rename(a, b)} operations to scale when \code{a} and \code{b}
point to different inodes, even though they commute.
\fs has a separate internal interface to check for existence of a
file name, without looking up the inode, which allows \code{access}
and \code{rename} to scale in such situations.


\subsection{Difficult-to-scale cases}

As \cref{fig:testcase-breakdown-sv6} illustrates, there are a few
(\pyexpr{mscan.xv6.shared} out of \pyexpr{mscan.ntestcases})
commutative test cases for
which \vm and \fs are not conflict-free.
%
The majority of these tests involve idempotent updates to internal
state, such as two \code{lseek} operations that both seek a file
descriptor to the same offset, or two anonymous \code{mmap} operations
with the same fixed base address and permissions.  While it is
possible implement these scalably, every implementation we considered
significantly impacted the performance of more common operations, so
we explicitly chose to favor common-case performance over total
scalability.  Even though we decided to forego scalability in these
cases, the commutativity rule and \tool forced us to consciously make
this trade-off.
%
\XXX[AC]{About 75\% are known idempotent.}

% Non-idempotent shared case list (incomplete):
% close_close_pe_5: Closing pipe writer (eager counting)
% close_read_pdc_0: Closing pipe writer, reading writer always EBADF
% close_write_pd8_0: Symmetric to close_read_pdc_0
% lseek_read_pad8_2: Reading nothing from two different offsets
% lseek_read_p8b6_3: Same as lseek_read_pad8_2
% mmap_mprotect_pf8_41: Making different read-only mappings read-only
%   (The mprotect is a no-op, but what it reads is modified)
% mmap_mprotect_pae_40: Same
% read_read_pce4_0: Two reads from a file full of zeroes
% read_write_pbb0_2: Reading nothing from different end-of-file offsets
% write_write_pff8_1: Writing to a pipe with no readers
%   (We could probably fix this one)
% write_write_pff8_2: Same
% write_write_pf78_1: Same

Other difficult-to-scale cases are more varied.  Several involve
reference counting of pipe file descriptors.  Closing the last file
descriptor for one end of a pipe must immediately affect the other
end; however, since there's generally no way to know a priori if a
\code{close} will close the pipe, a shared reference count is used in
some situations.  Other cases involve operations that return the same
result in either order, but for different reasons, such as two reads
from a file filled with identical bytes.

\XXX[AC]{fstest works around all radix tree expansion.  Is this
cheating?  That's actually an interesting case.}

\XXX[AC]{Summarize or something?}
