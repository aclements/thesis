\section{Finding scalability opportunities}
\label{sec:model}

To understand whether \tool{} is useful to kernel developers,
we modeled several POSIX file system and virtual memory calls in \tool,
then used this both to evaluate Linux's scalability and to develop a
scalable file and virtual memory system for our \sys research kernel.
%
The rest of this section uses this case study to answer the following
questions:

\begin{CompactItemize}

\item How many test cases does \tool{} generate, and what do they test?

\item How good are current implementations of the POSIX interface?
      Do the test cases generated by \tool{} find
      cases where current implementations don't scale?

\item What techniques are necessary to achieve scalability for
      cases where current file and virtual memory systems do not
      scale?

\item What situations might be too difficult or impractical to
      make scale, despite being commutative?

\end{CompactItemize}

\subsection{POSIX test cases}

To answer the first question, we developed a simplified model of the
POSIX file system and virtual memory APIs in \tool{}.  The model covers
\pyexpr{len(mscan.calls)} system
calls, and includes inodes, file
names, file descriptors and their offsets, hard links, link counts,
file lengths, file contents, file times, pipes, memory-mapped files,
anonymous memory, processes, and threads.  Our model
also supports nested
directories, but we disable them because Z3 does not currently handle
the resulting constraints.
\XXX[AC][Do we want to say any more about nested directories?]
%
We restrict file sizes and offsets to page granularity; some \sys data
structures are conflict-free for offsets on different pages, but
offsets within a page conflict.
%
\tool generates a total of \pyexpr{mscan.ntestcases} test cases
from our model.
%
Generating the test cases and running them on both Linux and \sys
takes a total of 8~minutes on the machine described in
\S\ref{sec:topic:ben}.

The model implementation and its model-specific test code generator
are \pyexpr{const['commuter-loc']['fsmodel']} and
\pyexpr{const['commuter-loc']['fstestgen']} lines of Python code,
respectively.
Figure~\ref{fig:rename-spec} showed a part of our
model, and Figure~\ref{fig:testgen} gave an example test
case generated by \tool{}.  We verified that all test cases return
the expected results on both Linux and \sys.

\begin{figure*}
\small
\centering
\input{figures/testcases}
\caption{
  Scalability for system call pairs, showing the fraction and number
  of test cases generated by \tool that are not
  conflict-free for each system call pair.
  One example test case was shown in Figure~\ref{fig:testgen}.
}
\label{fig:testcase-breakdown}
\end{figure*}

\XXX[AC][memwrite is weird because mscan ignores user space addresses,
so the actual write performed by memwrite isn't counted.  That's why
even the idempotent ones are ``conflict-free''.]

\subsection{Current implementation scalability}

To evaluate the scalability of existing file and virtual memory systems,
we used \mtrace{} to check the above test cases against
Linux kernel version 3.8.
Linux developers have invested significant effort in making the file
system scale~\cite{boyd-wickizer:scaling}, and it already scales in
many interesting cases, such as concurrent operations in different
directories or concurrent operations on different files in the same
directory that already exist~\cite{lwn:dcache}.
%
We evaluated the \code{ramfs} file system because
\code{ramfs} is effectively a user-space interface to the Linux buffer
cache.  Since exercising \code{ramfs} is equivalent to exercising the
buffer cache and the buffer cache underlies all Linux file
systems, this represents the best-case scalability for a Linux file
system.
%
Linux's virtual memory system, in contrast, involves process-wide
locks that are known to limit its
scalability and impact real
applications~\cite{boyd-wickizer:scaling,clements:bonsai,gil:c4}.

\XXX[AC][Maybe call out pure FS calls and/or pure VM calls?]
The left half of Figure~\ref{fig:testcase-breakdown} shows the results.
Out of \pyexpr{mscan.ntestcases} test cases,
\pyexpr{mscan.linux.shared} cases, widely distributed across the
system call pairs, were not conflict-free.
This indicates that even a mature and reasonably scalable operating system
implementation misses many cases that can be made to scale according
to the commutativity rule.

A common source of access conflicts is shared reference counts.
%
For example, most file name lookup operations update the reference
count on a \code{struct dentry}; the resulting write conflicts cause
them to not scale.
%
Similarly, most operations that take a file descriptor update the
reference count on a \code{struct file}, making
commutative operations such as two \code{fstat} calls on the
same file descriptor not scale.
Coarse-grained locks are another source of access conflicts.  For
instance, Linux locks the parent directory for any operation that
creates file names, even though operations that create distinct names
generally commute.
%
%Many of these problems affect application
%scalability~\cite{boyd-wickizer:scaling}.
%
Similarly, we see that coarse-grained locking in the virtual memory
system severely limits the conflict-freedom of address space manipulation
operations. This agrees with previous
findings~\cite{boyd-wickizer:scaling,clements:radixvm,clements:bonsai},
which demonstrated these problems in the context of several applications.

% Another source of non-scalability is
% needless lock acquisition: for instance, Linux locks an inode when
% truncating the file in \code{open(O_TRUNC)} even if the file is
% already empty, making concurrent \code{open(O_TRUNC)} calls on an existing
% empty file non-scalable.  A final example is needless updates: if two file
% names \code{a} and \code{b} point to the same inode, \code{rename(a, b)}
% need not modify the \code{dentry} for \code{b}, but Linux does so anyway.
% \XXX[nz][Weak examples; we should at least finish with an
% interesting/important case.]


\subsection{Making test cases scale}

Given that Linux does not scale in many cases, how hard is it
to implement scalable file systems and virtual memory systems?  To answer this
question, we designed and implemented a \code{ramfs}-like in-memory
file system called \fs and a virtual memory system called RadixVM for
\sys, our research kernel based on xv6~\cite{xv6}.  RadixVM appeared
in previous work~\cite{clements:radixvm}, so we focus on \fs here.
%
Although it is in principle possible to make the same changes in Linux,
we chose not to implement \fs in Linux because \fs's design
would have required modifying code throughout the Linux kernel.
%
The designs of both RadixVM and \fs were guided by the
commutativity rule.  For \fs, we relied heavily on \tool throughout
development to guide its design and identify sharing problems in its
implementation (RadixVM
was built prior to
\tool).
%
The right half of Figure~\ref{fig:testcase-breakdown} shows the
result of applying \tool
to \sys.
\XXX[AC][Mention here or elsewhere that \tool was partly motivated by
the difficulty of building a scalable file system?]

\fs makes extensive use of existing techniques for scalable
implementations, such as per-core resource
allocation, double-checked locking, lock-free readers using
RCU~\cite{rcu:linux},
scalable reference counts using Refcache~\cite{clements:radixvm},
and seqlocks~\cite[\S6]{lameter:linuxsync}.  These techniques lead to
several common patterns, as follows; we illustrate the patterns with
example test cases from \tool{} that led us to discover these situations:


\paragraph{Layer scalability.}  \fs uses data structures that
themselves naturally satisfy the commutativity rule, such as linear
arrays, radix arrays~\cite{clements:radixvm}, and hash tables.  In
contrast with structures like balanced trees, these data
structures
typically share no cache lines when different elements are accessed
or modified.  For example, \fs stores the cached data pages for a given inode
using a radix array, so that concurrent reads or writes to different
file pages scale, even in the presence of operations
extending or truncating the file.
% , as long as they don't access the part
% of the file that's being extended or truncated.
Many operations also use this radix array to determine if some offset
is within the file's bounds without risking conflicts with operations
that change the file's size.

\XXX[AC][Nir's book calls such data structures ``naturally parallel''
or something.]


\paragraph{Defer work.} Many kernel resources are shared,
such as files and pages, and must be freed when no longer referenced.
Typically, kernels release resources immediately, but this requires
eagerly tracking references to resources, causing
commutative operations that access the same resource to conflict.  Where
releasing a resource is not time-sensitive, \fs
uses Refcache~\cite{clements:radixvm} to batch reference count
reconciliation and zero detection.  This way, resources are eventually
released, but within each Refcache epoch commutative operations can be
conflict-free.

\XXX[AC][Discuss crazy pipe hybrid reference counting scheme?]

\XXX[AC][This seems uninteresting, and possibly false. \par
Batching sometimes enables absorption, when a resource gets reused within
a batch interval, thus requiring no non-scalable operations at all.
For instance, if one process closes the last open file descriptor for
a file, that file becomes eligible for eviction from the inode cache
(using Refcache weak references~\cite{clements:radixvm}).  However,
if another process re-opens the file on the same core, the inode can
be revived without any conflicting memory accesses.]

Some resources are artificially scarce, such as inode numbers in a typical
Unix file system.  When a typical Unix file system runs out of free
inodes, it must reuse an inode from a recently deleted file.  However,
the POSIX interface does not require that inode numbers be reused, only
that the same inode number is not used for two files at once.  Thus,
\fs never reuses inode numbers.  Instead, inode numbers are
generated by a monotonically increasing per-core counter, concatenated
with the core number that allocated the inode.  This allows \fs to defer
inode garbage collection for longer periods of time, and enables scalable
per-core inode allocation.


% \paragraph{Check before updating.}  Before updating any data structure,
% \fs first checks whether the data structure already contains the new
% value, and avoid any writes if so.  For example, the Linux file system
% always acquires a lock on an inode when truncating the file.  This means
% that concurrent truncate operations do not scale, even if the file
% is already empty.  \fs first checks the current length of the file,
% and avoids updating the length or acquiring any locks if the file is
% already at the right size.

% As another example, if one thread tries to create an existing file using
% \code{open(O_CREAT|O_EXCL)}, it should fail, but a na\"ive implementation
% might first acquire a lock to prepare for creating the file, and then
% check whether the file already exists.  This makes
% concurrent \code{open(O_CREAT|O_EXCL)} calls for an existing file name
% non-scalable.  \fs first checks whether the file already exists without
% modifying any cache lines, making these operations scale.

% \XXX[AC][The above two examples are slightly off.  E.g., if I do two
% truncates and the first writes to the file length and the second reads
% that, that's sharing.  These are both idempotent updates, which is one
% of the few things we \emph{don't} do scalably.]

% As a final example, if two file names \code{a} and \code{b} point to the
% same inode, \code{rename(a, b)} should remove the directory entry for
% \code{a}, but it should not modify the directory entry for \code{b}, since
% it already points at the right inode.  By checking the directory
% entry for \code{b} before updating it, \fs allows \code{rename(a, b)}
% to scale with other operations that look up \code{b}.


\paragraph{Precede pessimism with optimism.} Many operations
in \fs have an optimistic check stage followed by a pessimistic update
stage, a generalized sort of double-checked locking.  The optimistic
stage checks conditions for the operation and returns immediately if
no updates are necessary (this is often the case for error returns,
but can also happen for success returns).  This stage does no writes
or locking, but because no updates are necessary, it is often easy to
make atomic.  If updates are necessary, the operation acquires
locks or uses lock-free protocols, re-verifies its conditions to
ensure atomicity of the update stage, and performs updates.  For
example, \code{lseek} first computes the new offset using a lock-free
read-only protocol and returns early if the new offset is invalid or
equal to the current offset.  Otherwise, \code{lseek} locks the file
offset, and re-computes the new offset to ensure consistency.

\code{rename} is similar.  If two file names \code{a} and \code{b}
point to the
same inode, \code{rename(a, b)} should remove the directory entry for
\code{a}, but it does not need to modify the directory entry for
\code{b}, since
it already points at the right inode.  By checking the directory
entry for \code{b} before updating it, \code{rename(a, b)} avoids
conflicts with other operations that look up \code{b}.


\paragraph{Don't read unless necessary.}  A common internal interface
in a file system implementation is a \code{namei} function that
checks whether a path name exists, and if so, returns the inode for
that path.
%
However, reading the inode is unnecessary
if the caller wants to know only whether a path name existed, such as
an \code{access(F_OK)} system call.  In particular, the \code{namei}
interface makes it impossible for concurrent \code{access(b, F_OK)}
and \code{rename(a, b)} operations to scale when \code{a} and \code{b}
point to different inodes, even though they commute.
\fs has a separate internal interface to check for existence of a
file name, without looking up the inode, which allows \code{access}
and \code{rename} to scale in such situations.


\subsection{Difficult-to-scale cases}

As Figure~\ref{fig:testcase-breakdown} illustrates, there are a few
(\pyexpr{mscan.xv6.shared} out of \pyexpr{mscan.ntestcases})
commutative test cases for
which \fs is not conflict-free.
%
The majority of these tests involve idempotent updates to internal
state, such as two \code{lseek} operations that both seek a file
descriptor to the same offset, or two anonymous \code{mmap} operations
with the same fixed base address and permissions.  While it is
possible implement these scalably, every implementation we considered
significantly impacted the performance of more common operations, so
we explicitly chose to favor common-case performance over total
scalability.  Even though we decided to forego scalability in these
cases, the commutativity rule and \tool forced us to consciously make
this trade-off.
%
\XXX[AC][About 75\% are known idempotent.]

% Non-idempotent shared case list (incomplete):
% close_close_pe_5: Closing pipe writer (eager counting)
% close_read_pdc_0: Closing pipe writer, reading writer always EBADF
% close_write_pd8_0: Symmetric to close_read_pdc_0
% lseek_read_pad8_2: Reading nothing from two different offsets
% lseek_read_p8b6_3: Same as lseek_read_pad8_2
% mmap_mprotect_pf8_41: Making different read-only mappings read-only
%   (The mprotect is a no-op, but what it reads is modified)
% mmap_mprotect_pae_40: Same
% read_read_pce4_0: Two reads from a file full of zeroes
% read_write_pbb0_2: Reading nothing from different end-of-file offsets
% write_write_pff8_1: Writing to a pipe with no readers
%   (We could probably fix this one)
% write_write_pff8_2: Same
% write_write_pf78_1: Same

Other difficult-to-scale cases are more varied.  Several involve
reference counting of pipe file descriptors.  Closing the last file
descriptor for one end of a pipe must immediately affect the other
end; however, since there's generally no way to know a priori if a
\code{close} will close the pipe, a shared reference count is used in
some situations.  Other cases involve operations that return the same
result in either order, but for different reasons, such as two reads
from a file filled with identical bytes.

\XXX[AC][fstest works around all radix tree expansion.  Is this
cheating?  That's actually an interesting case.]

\XXX[AC][Summarize or something?]
