\subsection{Refcache: Scalable reference counting}
\label{sec:sv6:refcache}

Reference counting is critical to many OS functions and used
throughout \vm and \fs.  \vm must reference count physical pages
shared between address spaces, such as when forking a process, as well
as nodes in its internal radix tree.  Likewise, \fs must reference
count file descriptions, FD tables, and entries in various caches as
well as scalably maintain other counts such as link counts.
%
\Thiscref{sec:sv6:refcache} introduces \refcache, a novel reference
counting scheme
used by \vm and \fs.  \refcache implements space-efficient, lazy,
scalable reference counting using per-core \emph{reference delta
  caches}.  \refcache targets uses of reference counting that can
tolerate some latency in releasing reference counted resources and is
particularly suited to uses where increment and decrement operations
often occur on the same core (e.g., the same thread that allocated
pages also frees them).

% Since two virtual memory regions may share the same physical
% pages, such as when forking a process, \vm must have a way
% to decide when to free the underlying physical pages.  To do so,
% \vm reference counts each physical page, but a simple scheme with a
% single counter can result in scalability problems because threads will
% contend for the counter.  Likewise, \vm reference counts nodes of its
% radix tree to determine when they are empty; a single counter would
% cause operations on different parts of the node to contend.

Despite the seeming simplicity of reference counting, designing a
practical reference counting scheme that is conflict-free and scalable
for most operations turns out to be an excellent exercise in applying
the scalable commutativity rule to both implementation and interface
design.

We first touched on this problem in \cref{sec:rule:rc-example}, where
we introduced the notion of a \emph{lazy reference counter} that
separates manipulation from zero detection.  Simple reference counters
have two operations,
\code{inc($o$)} and \code{dec($o$)}, which both return the new
reference count.  These operations trivially commute when applied to
different objects, so we focus on the case of a single object.  When
applied to the same object, these operations \emph{never} commute (any
reordering will change their responses) and cannot be made
conflict-free.

A better (and more common) interface returns nothing from \code{inc}
and, from \code{dec}, returns only an indication of whether the count
is zero.  This is equivalent to the Linux kernel's \code{atomic_inc}
and \code{atomic_dec_and_test} interface and is echoed in software
ranging from Glib to Python.  Here, a sequence of \code{inc} and
\code{dec} operations commutes if (and only if) the count does not
reach zero in any reordering.  In this case, the results of the
operations will be the same in all reorderings and, since reordering
does not change the final sum, no future operations can distinguish
different orders.  Therefore, by the scalable commutativity rule, any
such sequence has some conflict-free implementation.  Indeed,
supposing a \emph{particular} history with a commutative region, an
implementation can partition the count's value across cores such that
no per-core partition drops to zero in the commutative region.
However, a practical, \emph{general-purpose} conflict-free
implementation of this interface remains elusive.

\refcache pushes this interface reduction further, targeting
reference counters that can tolerate some latency between when the
count reaches zero and when the system detects that it's reached zero.
%
% Like the interface changes presented in \cref{sec:posix}, \refcache
% is a further example of redesigning interfaces to enable greater
% implementation scalability.
% %
% \XXX[AC]{Emphasize here that this is suitable for many uses of
%   reference counting, like memory reclamation?}

In \refcache, \emph{both} \code{inc} and \code{dec} return nothing and
hence always commute, even if the count reaches zero.  A new
\code{review()} operation finds all objects whose reference counts
recently reached zero (and, in practice, calls their destructors).
\code{review} does not commute in any sequence where \emph{any}
object's reference count has reached zero and its implementation
conflicts on a small number of cache lines even when it does commute.
%
However, unlike \code{dec}, the system can control when and how often
it invokes \code{review}.
%
\sys invokes it
only at 10ms intervals---several orders of magnitude longer than the
time required by even the most expensive conflicts on current
multicores.
%
\refcache strikes a balance between broad conflict-freedom, strict
adherence to the scalable commutativity rule, and practical
implementation concerns, providing a general-purpose, efficient
implementation of an interface suitable for many uses of referencing
counting.

By separating count manipulation and zero detection, \refcache can
batch increments and decrements and
reduce cache line conflicts while offering an adjustable time bound
on when an object will be garbage collected after its reference count
drops to zero.  \code{inc} and \code{dec} are conflict-free with high
probability and \code{review} induces only a small constant rate of
conflicts for global epoch maintenance.

% XXX[AC]{I feel like these counters are different in too many ways to
% summarize neatly like this.  But maybe I chose the wrong columns?}
%
% \begin{figure}
%   \centering
%   \def\yes{Y}
%   \def\no{N}
%   \begin{tabular}{lcccccccc} \toprule
%     & Conflict-free & Zero-detection & Per-object & Immediate \\
%     & \code{inc}/\code{dec} & time & space & zero-detection \\
%     \midrule
%     %
%     Shared counters      & \no       & $O(1)$
%     & $O(1)$           & \yes \\
%     %
%     Distributed counters & \yes      & $O(\text{objs}\cdot\text{cpus})$
%     & $O(\text{cpus})$ & \no \\
%     %
%     SNZI                 & Partially & $O(1)$
%     & $O(\text{cpus})$ & \yes \\
%     %
%     Modula-2+ counts     & \yes      & $O(\text{heap})$
%     & $O(1)$           & \no \\
%     %
%     Sloppy counters      & \yes      & $O(\text{objs}\cdot\text{cpus})$
%     & $O(\text{cpus})$ & \no \\
%     %
%     \refcache            & \yes      & $O(1)$
%     & $O(1)$           & \no \\
%     \bottomrule
%   \end{tabular}
%   \caption{Comparison of reference counting techniques.}
%   \label{fig:refcache-comparison}
% \end{figure}

\refcache inherits ideas from sloppy
counters~\cite{boyd-wickizer:scaling}, Scalable NonZero Indicators
(SNZI)~\cite{snzi:podc}, distributed counters~\cite{appavoo:k42},
shared vs.\ local counts in Modula-2+~\cite{detreville:concurrent-gc},
and approximate counters~\cite{approx:counter}.
%
% Modula-2+ counters are in section 2.3 and 2.5 of the Modula-2+ paper
%
All of these techniques speed up increment and decrement operations
using per-core counters, but make different trade-offs between
conflict-freedom, zero-detection cost, and space.  With the exception
of SNZIs, these techniques do not detect when a count reaches zero, so
the system must poll each counter for this condition.  SNZIs detect
when a counter reaches zero immediately, but at the cost of conflicts
when a counter's value is small.  \refcache balances these extremes, enabling
conflict-free operations and efficient zero-detection, but with a
short delay.
%
Furthermore, in contrast with sloppy, SNZI, distributed, and
approximate counters, \refcache
requires space proportional to the sum of the number of reference
counted objects and the number of cores, rather than the product, and
the per-core overhead can be adjusted to trade off space and
scalability by controlling the reference delta cache collision rate.
%
Space overhead is particularly important for \vm, which must reference
count every physical page; at large core counts, other scalable
reference counters
would require more than half of physical memory just to track the
remaining physical memory.

\subsubsection{Basic \refcache}
In \refcache, each reference counted object has a global reference
count (much like a regular reference count) and each core also
maintains a local, fixed-size cache of deltas to objects' reference
counts.  Incrementing or decrementing an object's reference count
modifies only the local, cached delta and \code{review} periodically
flushes this delta to the object's global reference count.
%
The true reference
count of an object is thus the sum of its global count and any local
deltas for that object found in the per-core delta caches.  The value of the
true count is generally unknown, but we assume that once the true
count drops to
zero, it will remain zero (in the absence of weak references, which
we discuss later).  \refcache depends on this stability to detect a
zero true count after some delay.

To detect a zero true reference count, \refcache divides time into
periodic \emph{epochs} during which each core calls \code{review} once
to flush all of the
reference count deltas in its cache and apply these updates to the
global reference count of each object.  The last core in an epoch to
finish flushing its cache ends the epoch and all of the cores repeat
this process after some delay (our implementation uses 10ms).  Since
these flushes occur in no particular order and the caches batch
reference count changes, updates to the reference count can be
reordered.  As a result, a zero global reference count does not imply
a zero true reference count.  However, once the true count \emph{is}
zero, there will be no more updates, so if the global reference count
of an object drops to zero and \emph{remains} zero for an entire
epoch, then \code{review} can guarantee that the true count is zero
and free the object.  To detect this, the first \code{review}
operation to set an object's global reference count to zero adds the
object to the local per-core \emph{review queue}.  \code{Review} then
reexamines it two epochs later (which
guarantees one complete epoch has elapsed) to decide whether its true
reference count is zero.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/refcache.pdf}
  %
  \splitcaption{\refcache example showing a single object over eight
    epochs.}{Plus and minus symbols represent increment and decrement
    operations, dotted lines show when cores flush these to the object's
    global count, and blue circles show when each core flushes its
    local reference cache.  The loops around the global count show
    when the object is in core 0's review queue and
    the red zeroes indicate dirty zeroes.}
  \label{fig:refcache-ex}
\end{figure*}

\Cref{fig:refcache-ex} gives an example of a single object over
the course of eight epochs.  Epoch 1 demonstrates the power of
batching: despite six reference count manipulations spread over three
cores, all \code{inc} and \code{dec} operations are conflict-free (as
we would hope, given that they commute), the object's global reference
count is never written to and
cache line conflicts arise only from epoch maintenance.  The
remaining epochs demonstrate the complications that arise from
batching and the resulting lag between the true reference count and
the global reference count of an object.

Because of the flush order, the \code{inc} and \code{dec} in epoch 2
are applied to
the global reference count in the opposite order of how they actually
occurred.  As a result, core 0 observes the global count temporarily
drop to zero when it flushes in epoch 2, even though the true count is
non-zero.  This is remedied as soon as core 1 flushes its increment,
and when core 0 reexamines the object at the beginning of epoch 4,
after all cores have again flushed their delta caches, it can see
that the global count is non-zero; hence, the zero count it observed
was not a true zero and the object should not be freed.

It is necessary but not sufficient for the global reference count to
be zero when an object is reexamined; there must also have been no
deltas flushed to the object's global count in the interim, even if
those changes canceled out or the deltas themselves were zero.
%
For example, core 0's \code{review} will observe a zero global
reference count
at the end of epoch 4, and again when it reexamines the object in
epoch 6.  However, the true count is not zero, and the global
reference count was temporarily non-zero during the epoch.  We call
this a \emph{dirty} zero and in this situation \code{review} will
queue the object to be examined again two epochs later, in epoch 8.

\subsubsection{Weak references}
As described, \refcache is well suited to reference counts that track
the number of true references to an object, since there is no danger
of the count going back up from zero once the object becomes unreachable.
However, operating systems often need untracked references to objects;
for example, \fs's caches track objects that may be deleted at any time,
and may even need to bring an object's reference count back up from
zero.  \vm's radix tree has similar requirements.  To support such
uses, we extend \refcache with \emph{weak references}, which provide a
\code{tryget} operation that will either increment the object's
reference count (even if it has reached zero) and return the object,
or will indicate that the object has already been deleted.

A weak reference is simply a pointer marked with a ``dying'' bit,
along with a back-reference from the referenced object.  When an
object's global reference count initially reaches zero, \code{review} sets
the weak reference's dying bit.  After this, \code{tryget} can
``revive'' the object by atomically clearing the dying bit and
fetching the pointer value, and then incrementing the object's
reference count as usual.  When \code{review} decides to free an object,
it first atomically clears both the dying bit and the pointer in the
weak reference.  If this succeeds, it can safely delete the object.
If this fails, it reexamines the object again two epochs later.  In a
race between \code{tryget} and \code{review}, whether the object is
reviewed or deleted is determined by which operation clears the dying
bit first.

This protocol can be extended to support multiple weak references per
object using a two phase approach in which \code{review} first puts
all weak references to an object in an intermediate state that can be
rolled back if any
reference turns out to be revived.  Our current implementation does
not support this because it is unnecessary for \vm or \fs.

\subsubsection{Algorithm}
\Cref{fig:refcache-code} shows pseudocode for \refcache.  Each core
maintains a hash table
storing its reference delta cache and the review queue that tracks
objects whose global reference counts reached zero.  A core reviews an
object after two epoch boundaries have passed, which guarantees
that all cores have flushed their
reference caches at least once.

\XXX![AC]{Make pseudo-code formatting consistent with rule chapter.}

\begin{figure}
  \def\fgap{-0.25em}            % Fit on a page
  \begin{tabbing}
    \quad\=\quad\=\quad\=\quad\=\quad\=\quad\=\kill
    inc(obj): \+\\
      if local-cache[hash(obj)].obj $\ne$ obj: \+\\
        evict(local-cache[hash(obj)]) \\
        local-cache[hash(obj)] $\gets$ $\langle$obj, 0$\rangle$ \-\\
      local-cache[hash(obj)].delta += 1 \\
    \-\\[\fgap]
    tryget(weakref): \+\\
      do: \+\\
        $\langle$obj, dying$\rangle$ $\gets$ weakref \-\\
      while weakref.cmpxchg($\langle$obj, dying$\rangle$,
        $\langle$obj, false$\rangle$) fails \\
      if obj is not null: \+\\
        inc(obj) \-\\
      return obj \\
    \-\\[\fgap]
    evict(obj, delta): \+\\
      % If this condition is true, then the locked region would be a
      % no-op, but why is it safe to do it without the lock?  The
      % danger is that the read of refcnt will slip in the middle of
      % another region that modified refcnt with obj locked.  evict
      % is the only place this happens and it does only a single write
      % to refcnt.  Assuming both this read and that write are
      % atomic, then we can consider this check atomic with the entire
      % locked region in the competing evict.
      if delta = 0 and obj.refcnt $\ne$ 0: return \\
      with obj locked: \+\\
        obj.refcnt $\gets$ obj.refcnt + delta \\
        if obj.refcnt = 0: \+\\
          if obj is not on any review queue: \+\\
            obj.dirty $\gets$ false \\
            obj.weakref.dying $\gets$ true \\
            add $\langle$obj, epoch$\rangle$ to the local review
            queue \-\\
          else: \+\\
            obj.dirty $\gets$ true \\
    \-\-\-\-\\[\fgap]
    flush(): \+\\
      evict all local-cache entries and clear cache \\
      update the current epoch \\
    \-\\[\fgap]
    review(): \+\\
      flush() \\
      for each $\langle$obj, objepoch$\rangle$ in local review queue: \+\\
        if epoch $<$ objepoch + 2: continue \\
        with obj locked: \+\\
          remove obj from the review queue \\
          if obj.refcnt $\ne$ 0: \+\\
            obj.weakref.dying $\gets$ false \-\\
          else if obj.dirty or obj.weakref.cmpxchg(%
                $\langle$obj, true$\rangle$,
                $\langle$null, false$\rangle$) fails: \+\\
            obj.dirty $\gets$ false \\
            obj.weakref.dying $\gets$ true \\
            add $\langle$obj, epoch$\rangle$ to the local review queue \-\\
          else: \+\\
            free obj
  \end{tabbing}
  \vspace{-1em}                 % No idea where this space comes from
  \rule{\columnwidth}{0.5pt}
  \vspace{-\baselineskip}
  \caption[\refcache algorithm.]
  {\refcache algorithm.  Each core calls \code{review}
    periodically.  \code{evict} may be called by \code{flush} or because of a
    collision in the reference cache.  \code{dec} is identical to \code{inc} except
    that it decrements the locally cached delta.}
  \label{fig:refcache-code}
\end{figure}

All of the functions in \cref{fig:refcache-code} execute with
preemption disabled, meaning they are atomic with respect to each
other on a given core, which protects the consistency of per-core data
structures.  Fine-grained locks protect the \refcache-related fields
of individual objects.

For epoch management, our current implementation uses a barrier scheme
that tracks a global epoch counter, per-core epochs, and a count of
how many per-core epochs have reached the current global epoch.  This
scheme suffices for our benchmarks, but schemes with fewer cache-line
conflicts are
possible, such as the tree-based quiescent state detection used
by Linux's hierarchical RCU implementation~\cite{lwn:treercu}.

\subsubsection{Discussion}
\refcache trades latency for scalability by batching increment and
decrement operations in per-core caches.  As a result, except when
collisions in the reference delta cache cause evictions, \code{inc}
and \code{dec} are naturally conflict-free with all other operations.
%
Furthermore, because \refcache uses per-core caches
rather than per-core counts, it is more space-efficient than other
scalable reference counting techniques.  While not all uses of
reference counting can tolerate \refcache's latency, its scalability
and space-efficiency are well suited to the requirements of \vm and
\fs.

% \XXX[Austin]{Design alternatives: token-passing or broadcast IPI for
% epoch detection; lock-free versus fine-grained locking?}
