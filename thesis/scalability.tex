\section{Scalability and conflict-freedom}
\label{sec:scalability}

\XXX![AC]{Write me.}

% Directory-based

% Interconnect

% cc-NUMA

% Should I explain cache coherence and directories and such here or
% assume that's understood?  How ``introductory'' should this chapter
% be?

% What really matters is what *doesn't* require coordination in the
% steady-state.  What can be done purely locally.  Any transition that
% involves coordination represents a scalability hazard.

We prove the connection between commutativity and conflict-free
implementations in \cref{sec:rule}, but what ultimately matters to
practitioners is scalability.  This chapter shows that
conflict-freedom is a good predictor of linear scalability on modern
multicore hardware, closing the gap between the theoretical basis of
the scalability commutativity rule and its utility in practice.

The connection between conflict-freedom and scalability mustn't taken
for granted.  Indeed, some early multi-processor architectures such as
the Intel Pentium depended on shared buses with global lock
lines~\cite[\S8.1.4]{intel-sdm-3}, so even conflict-free operations
did not scale.

Today's multicores avoid these centralized components.  Modern, large,
cache-coherent multicores utilize peer-to-peer interconnects between
cores, partitioned physical memory (NUMA), and per-core write-back
caches.
%
To maintain a unified, globally consistent view of memory despite
their distributed architecture, multicores depend on MESI-like
coherence protocols~\cite{papamarcos:mesi} to coordinate ownership of
cached memory.
%
A key invariant of these coherence protocols is that a cache line is
either not present in any cache, a mutable copy is present in a single
cache, or it is present in any number of caches but is immutable.
%
Maintaining this invariant requires coordination, and this is where
the connection to scalability lies.

\begin{figure}
  \centering
  \begin{tikzpicture}[x=2cm,y=2cm,bend angle=10,
%    every node/.style={draw},
    rfo/.style={->,red,ultra thick},
    local/.style={->,green!50!black,thick},
    remote/.style={->,dotted}]

    \begin{scope}[every node/.append style={shape=ellipse}]
      \node (I) at (90:1) {invalid};
      \node (S) at (200:1) {shared};
      \node (M) at (340:1) {modified};
    \end{scope}

    \begin{scope}[every node/.append style={shape=circle,inner sep=2pt}]
      \draw[rfo] (I) to[bend right] node[auto=right] {R} (S);
      \draw[rfo] (I) to[bend left] node[auto=left] {W} (M);

      \draw[rfo] (S) to[bend right] node[auto=right] {W} (M);
      \draw[local,<-] (S) to[loop left,distance=5mm] node {R} (S);
      \draw[remote] (S) to[bend right] node[auto=right] {rW} (I);

      \draw[local] (M) to[loop right,distance=5mm] node {R/W} (M);
      \draw[remote] (M) to[bend left] node[auto=left] {rW} (I);
      \draw[remote] (M) to[bend right] node[auto=right] {rR} (S);
    \end{scope}
  \end{tikzpicture}
  %
  \splitcaption{A basic cache-coherence state machine.}{``R'' and
    ``W'' indicate local read and write operations, while ``rR'' and
    ``rW'' indicate remote read and write operations.  Thick red lines
    show operations that cause communication.  Thin green lines show
    operations that occur without communication.}
  \label{fig:mesi}
\end{figure}

\Cref{fig:mesi} shows the basic state machine implemented by each
cache for each cache line.  This maintains the key invariant by
ensuring a cache line is either invalid in all caches, modified in one
cache and invalid in all others, or shared by any number of caches.
Practical implementations add further states---MESI's ``exclusive''
state, Intel's ``forward'' state~\cite{goodman:mesif}, and AMD's
``owned'' state~\cite[\S7.3]{amd-arch-2}---but these do not change the
basic communication requirements of cache-coherence.

Roughly, a set of operations scales when the coherence protocol does
not require communication in the steady state.  This is precisely when
those operations are conflict-free.  Multiple cores reading the same
cache line scales because the line can be stored in each reading
core's cache, allowing the reads to proceed independently and in
parallel with no coordination.  Likewise, a single core reading or
writing a cache line that is not read or written by any other core
also scales because, once that line is exclusive to that core's cache,
no coordination is required to access it, so no concurrent operations
will slow down the access.

Furthermore, any state transition that does involve coordination
represents a scalability hazard.  If a cache line is written by one
core and read or written by other cores, those operations must
coordinate and, as a result, will slow each other down.  While this
doesn't directly concern the scalable commutativity rule (which says
only when operations can be conflict-free, not when they must be
conflicted), the huge effect that conflicts can have on scalability
reaffirms our focus on conflict-freedom.

\Cref{sec:scalability:conflict-free} verifies that conflict-free
operations scale on real hardware and examines the cost of conflicting
operations, while \cref{sec:scalability:limits} explores the practical
limitations of conflict-freedom.


\subsection{Conflict-free operations scale}
\label{sec:scalability:conflict-free}

We use two machines to evaluate conflict-free and conflicting
operations on real
hardware: an 80-core (8 sockets $\times$ 10 cores) Intel Xeon E7-8870
(the same machine used for evaluation in \cref{sec:eval}) and a
48-core (8 sockets $\times$ 6 cores) AMD Opteron 8431.  Both are
cc-NUMA x86 machines with directory-based cache coherence, but the
architecture, interconnect, and coherence protocol
differs between the manufacturers.  \Cref{fig:machines} shows how the
two machines are broadly organized.

\begin{figure}
  \centering
  \XXX!{Interconnect, memory, and cache layout.}
  \caption{Organization of Intel and AMD machines.}
  \label{fig:machines}
\end{figure}

\XXX[AC]{The conflict-free benchmarks wait between accesses.  Maybe
  they should be run at full tilt?}

\XXX[AC]{The shading on cfree should be discretized.}

\begin{figure}
  \centering
  \input{graph/cfree-cycles}
  %
  \splitcaption{Conflict-free accesses scale.}{Each graph shows the
    cycles required to perform a conflict-free read or write from $N$
    cores.  Shading indicates the latency distribution for each $N$.}
  \label{fig:cfree-cycles}
\end{figure}

\Cref{fig:cfree-cycles} shows the time required to perform
conflict-free memory accesses from varying numbers of cores.  The
first benchmark, shown in the top row of \cref{fig:cfree-cycles},
stresses read/read sharing by repeatedly reading the same cache line
from $N$ cores.  The latency of these reads remains roughly constant
regardless of $N$ (growing slightly on the AMD machine because of
increasing noise).  After the first access from each core, the cache
line remains in each core's local cache, so later accesses occur
locally and independently, allowing read/read accesses to scale
perfectly.  Reads of different cache lines from different cores (not
shown) yield identical results to reads of the same cache line.

The bottom row of \cref{fig:cfree-cycles} shows the results of
stressing conflict-free writes by assigning each core a different
cache line and repeatedly writing these cache lines from each of $N$
cores.  In this case these cache lines enter a ``modified'' state at
each core, but then remain in that state, so as with the previous
benchmark, further writes can be performed locally and independently.
As a result, latency remains constant regardless of $N$, demonstrating
the conflict-free write accesses scale.

\begin{figure}
  \centering
  \input{graph/conflict-cycles}
  %
  \splitcaption{Conflicting accesses do not scale.}{Each graph shows
    the cycles required to perform a conflicting read or write from
    $N$ cores.  Shading indicates the latency distribution for each
    $N$ (estimated using kernel density estimation).}
  \label{fig:conflict-cycles}
\end{figure}

\Cref{fig:conflict-cycles} turns to the cost of conflicting
accesses.  The top row shows the latency of $N$ cores writing the same
cache line simultaneously.  The cost of a write/write conflict grows
dramatically as the number of writing cores increases because
ownership of the modified cache line must pass through each writing
core, one at a time.  On both machines, we also see a uniform
distribution of write latencies, which further illustrates this
serialization.
% On the AMD machine, the growth of the mean latency and the spread of
% the distribution are noticeably super-linear as the benchmark adds
% cores further from core 0.

The bottom row of \cref{fig:conflict-cycles} shows the latency of $N$
cores simultaneously reading a cache line last written by core 0 (a
read/write conflict).  For the AMD machine, the results are nearly
identical to the write/write conflict case, since this machine
serializes requests for the cache line at the home socket.  On the
Intel machine, the cost of read/write conflicts also grows with the
number of readers, as all readers must ultimately contact the home
socket for this cache line.  However, Intel's architecture aggregates
these requests at each socket, causing the cost of read/write
conflicts to increase in jumps up as the benchmark adds more sockets.
We see this effect in the latency distribution, as well, as requests
fall in to one of several latency modes based on when their socket's
aggregated read request arrives at the home socket (notably, all
sockets exhibit all modes, so this is not simply an effect of distance
from the home socket).  Intel's optimization helps reduce the absolute
latency of reads, but nevertheless, read/write conflicts do not scale
on either machine.


\subsection{Limitations of conflict-freedom}
\label{sec:scalability:limits}

\XXX![AC]{Write me.  Cache capacity (and associativity), directory
  capacity, compulsory misses (we assume a useful level of cache
  locality), false sharing.  Put in the benchmark from the OSDI paper,
  which covers the limitations.}

Conflict-freedom is a good predictor of scalability on real hardware,
but it's not perfect.  In practice, limits on cache capacity and
associativity can cause cache lines to be evicted from a cache even in
the absence of coherence traffic.  Such misses directly affect
sequential performance, but they may also affect the scalability of
conflict-free operations.




So far, we've largely ignored \emph{compulsory misses}, which occur
the first time a core accesses a cache line, \emph{capacity misses}...

% Causes home node access.  If the missing core is not the line's home
% node, then it has to make a remote request, which takes longer
% depending on how far away the remote node is and can contend for
% interconnect and the remote memory controller.  Even if a core
% misses on a local line, it may contend with other cores on the same
% node for resources.

% Assume some cache locality so compulsory misses are rare (enough to
% keep DRAM bandwidth demand within the capacity of the hardware),
% NUMA-aware memory allocation so that, when there is a miss, it can
% generally be serviced locally.  These things are standard sequential
% performance best practice, as well.


So far, we've treated each cache line as isolated and affected only by
reads and writes of that cache line.  But there are more cache lines
than there is space in a cache (otherwise it wouldn't be a cache), and
limits on cache capacity and associativity can cause cache lines to be
evicted from a cache, even in the absence of coherence traffic.  This
affects not only sequential performance, but scalability as well.
