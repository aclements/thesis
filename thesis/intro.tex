\section{Introduction}
\label{sec:intro}

\XXX[STATUS]{Re-organized and expanded from SOSP.  Draft almost
  ready.}

This dissertation presents a pragmatic and formal approach to the
design and implementation of scalable multicore software that spans
from the earliest stages of software interface conception through
testing and maintenance of complete implementations.

The rest of \thiscref{sec:intro} introduces the multicore
architectures that have taken over general-purpose computing, the
problematic ways in which software developers are coping with these
new architectures, and our solution to the design and implementation
of software for multicore architectures.


%\subsection{The rise of multicore architectures}
\subsection{Parallelize or perish}

Until the mid-2000's, writing higher performance software got easier
with every passing year as each new CPU model clambered for higher
clock speeds and greater computational throughput.
%
But higher clock speeds require more power and generate more heat, and
around 2005 clock speeds brought CPUs to the thermal dissipation
limits of a few square centimeters of silicon.
%
CPU architects could no longer significantly increase clock speeds, so
instead they began increasing parallelism, putting more cores on the
same chip, providing more and more independent processing units, while
maintaining shared access to memory and I/O resources.
% maintaining a ``coherent'' view of memory and shared access to I/O
% resources.
%
% With the rise of multicore architectures, parallel programming went
% from niche to necessary.
With the widespread adoption of multicore architectures for
general-purpose computing, parallel programming went from niche to
necessary.
%
All software must now be parallel to perform.
%
But, while software performance naturally scales with clock speed,
scaling with parallelism is an untamed problem.  Even with careful
engineering, software rarely achieves the holy grail of \emph{linear
  scalability}, where doubling the hardware parallelism doubles the
software's performance.

Operating system kernels are a poignant example of both the importance
of parallelism and the difficulty of achieving it.
%
As the common substrate that provides shared services and resources
that most applications depend heavily on, any limits on kernel
scalability become limits on application scalability.
%
Kernels must deal with interactions within and between applications
and cope with diverse and unknown workloads.
%
% \XXX{example?  Any application that creates, opens, read, or writes
%   files is at risk of contending with any other application that uses
%   the file system if the kernel's implementation is insufficiently
%   parallel.}
%
Underlining the importance of kernel scalability, Linux kernel
developers have put vast amounts of effort into multicore Linux,
bringing it to the forefront of putting parallel programming
techniques into practice.  Lock-free data structures, read-copy-update
garbage collection, per-core data, and sophisticated synchronization
mechanisms are all commonplace in the Linux kernel.

Yet, despite the extensive efforts of kernel and application
developers alike, scaling software performance on multicores remains
an inexact science dominated by guesswork, measurement, and expensive
cycles of redesign.
%
The state of the art for evaluating and improving the scalability of
multicore software is the choose some workload, plot performance at
varying numbers of cores, and use tools such as differential
profiling~\cite{mckenney:differential} to identify scalability
bottlenecks.  \XXX[AC]{Should we cite anything for this?  Is there
  anything to cite besides our own papers?}  \XXX[AC]{Mention that
  this is an evolution of sequential optimization techniques?}
%
This focuses developer effort on demonstrable issues, but is
ultimately a near-sighted approach.
%
% Different workloads and higher core counts often reveal new
% bottlenecks, leading to a Sisyphean cycle of performance catch-up and
% leaving workloads and hardware that kernel developers don't have
% access to to fend for themselves.
%
Each new hardware model and workload powers a Sisyphean cycle of
finding and fixing new bottlenecks.
%
Projects such as Linux require continuous infusions of manpower to
maintain their scalability edge and even then, workloads and hardware
that kernel developers don't have access to are a scalability gamble.
%
But the deeper problem with this workload-driven approach is that many
scalability problems lie not in the implementation, but in the
software interface.  By the time developers have an implementation, a
workload, and the hardware to demonstrate a bottleneck,
interface-level solutions may be impractical or impossible.

Consider the POSIX~\cite{posix2013} \code{open} call, which opens a
file by name and returns a \emph{file descriptor}, a number used to
identify the open file in later operations.
%
Even though this identifier is opaque, POSIX requires that \code{open}
return the numerically lowest available file descriptor for the
calling process, forcing the kernel to serialize file descriptor
allocation between threads in a process.
%
This simplified the kernel interface during the early days of Unix,
but this interface design choice is now a burden on implementation
scalability.
%
It's an unnecessary burden, too: a simple change to allow \code{open}
to return \emph{any} available file descriptor would enable the kernel
to choose file descriptors scalably.
%
This particular example is well-known~\cite{boyd-wickizer:corey}, but
myriad subtler issues exist in POSIX and other interfaces.

Interface design choices have implications for implementation
scalability.
%
If interface designers could distinguish interfaces that definitely
have a scalable implementation from those that don't, they would have
the predictive power to design \emph{scalable interfaces} that enable
scalable implementations.


\subsection{A rule for interface design}

This dissertation presents a new approach to scalability that starts
at scalable software interface design.
%
This makes reasoning about multicore scalability possible before an
implementation exists and before the necessary hardware is available
to measure the implementation's scalability.  It can highlight
inherent scalability problems, leading to alternate interface designs.
And it sets a clear scaling target for the implementation of a
scalable interface and enables systematic testing of an
implementation's scalability.

Scalability is often considered an implementation property, not an
interface property, not least because what scales depends on hardware.
%
However, modern shared-memory multicores use local caches and
protocols like MESI~\cite{papamarcos:mesi} to provide a globally
consistent ``coherent'' view of memory.  This general architecture
enables general arguments about scalability.
%
On such processors, a core can scalably read and write data it has
cached exclusively, and scalably read data it has cached in shared
mode. Writing a cache line that was last read or written by another
core is not scalable, however, since the coherence protocol serializes
ownership changes for each cache line, and because the shared
interconnect may serialize unrelated transfers.

We therefore say that a set of operations scales if their
implementations have \emph{conflict-free} memory accesses; that is, no
core writes a cache line that was read or written by another core.
%
When memory accesses are conflict-free, adding more cores will produce
a linear increase in capacity.
%
This is not a perfect model of the complex realities of modern
hardware, but it is a good approximation that we confirm in
\cref{sec:scalability}.

At the core of our approach is this \emph{scalable commutativity
  rule}: In any situation where several operations
\emph{commute}---meaning there's no way to distinguish their execution
order using the interface---they have an implementation whose memory
accesses are conflict-free during those operations.
%
Or, more concisely, \textbf{whenever interface operations commute,
  they can be implemented in a way that scales.}

This rule makes intuitive sense: when operations commute, their
results (return value and effect on system state) are independent of
order.  Hence, communication between commutative operations is
unnecessary, and eliminating it yields conflict-free and scalable
implementations.

The commutativity rule leads to a new way to design scalable
software:
%
analyze the interface's commutativity; if possible, refine or redesign
the interface to improve its commutativity, and then design an
implementation that scales in commutative situations.
%
For example,
consider file creation in a POSIX-like file system. Imagine that
multiple processes create files in the same directory at the same
time. Can the creation system calls be made to scale? Our
first answer was ``obviously not'': the system calls modify the same
directory, so surely the implementation must
serialize access to the directory. But it turns
out these operations commute if the two files have different names
(and no hard or symbolic links are involved) and, therefore, have an
implementation that scales for such names.
One such implementation represents each directory as a hash table
indexed by file name, with an independent lock per bucket,
so that creation of differently named files is conflict-free, barring
hash collisions.
%
Before the rule, we tried to determine if these
operations could scale by analyzing all of the \emph{implementations}
we could think
of.  This process was difficult, unguided, and itself did not scale to
complex interfaces, which
motivated our goal of reasoning about
scalability in terms of interfaces.


\subsection{Contributions}

\XXX{Should more of this stuff go into the rule section and this just
  be a summary?}

The rest of this dissertation presents the scalable commutativity rule
in depth and explores its consequences from interface design to
implementation to testing.

The intuition for the rule presented above is useful in practice, but
not precise enough to reason
about formally.
%
Therefore, the first contribution of this dissertation is a
formalization of the scalable commutativity rule and a proof of the
correctness of this formalized form, which we present in
\cref{sec:rule}.
%
Connecting this formalization to scalability as traditionally measured
requires certain assumptions about multicore hardware, which we verify
experimentally in \cref{sec:scalability}.

An important consequence of this presentation is a novel form of
commutativity we name \emph{\SIM\ commutativity}.
%
The usual definition of commutativity (e.g., for algebraic operations)
is so stringent that it rarely applies to the complex, stateful
interfaces common in systems software.
%
\SIM\ commutativity, in contrast, is \emph{state-dependent} and
\emph{interface-based}, as well as \emph{monotonic}.
%
When operations commute in the context of a specific system state,
specific operation arguments, and specific concurrent operations, we
show that an implementation exists that is conflict-free \emph{for that state
  and those arguments and concurrent operations}.
%
This exposes many more opportunities to apply the rule to real
interfaces---and thus discover scalable implementations---than a more
conventional notion of commutativity would.
%
Despite its logical state dependence, \SIM\ commutativity is
interface-based: rather than requiring all operation orders to produce
identical internal states, it requires the resulting states to be
indistinguishable via the interface.
%
\SIM\ commutativity is thus independent of any specific
implementation, enabling developers to apply the rule directly to
interface design.

Putting the rule into practice, in \cref{sec:posix} this dissertation
contributes a set of general guidelines for designing interfaces that
enable scalable implementations, inspired by both the successes and
failures of POSIX.  In these guidelines, we propose specific
modifications to POSIX that would allow for greater scalability.

However, complex interfaces are still complex, and even given the rule
and these interface design guidelines, it can be difficult to spot and
reason about all commutative cases.
%
% Complex interfaces can make it difficult to spot and reason about all
% commutative cases even given the rule.
%
To address this challenge, this dissertation introduces a method to
automate reasoning about interfaces and implementations, embodied in a
software tool named \tool, presented in \cref{sec:tool}.
%
\tool takes an interface model
in the form of a simplified, symbolic implementation, computes precise
conditions under which sets of operations commute, and tests an
implementation for conflict-freedom under these conditions.  This tool can be
integrated into the software development process to drive initial design and
implementation, to incrementally improve existing implementations, or to
help developers understand the commutativity of an interface.

In \cref{sec:topic:model}, we apply \tool to a simplified model of
\pyexpr{len(mscan.calls)} POSIX file system and virtual memory system
calls.
%
From this model,
\tool generates \pyexpr{mscan.ntestcases} tests of commutative
system call pairs, all of which can be made conflict-free
according to the rule.
%
Applying this suite to Linux, we find that the Linux kernel is
conflict-free for \pyexpr{mscan.ntestcases - mscan.linux.shared}
(\pyexpr{percent(1-(mscan.linux.shared/float(mscan.ntestcases)))}) of
these cases.
%
Many of the commutative cases where Linux is not conflict-free are
important to applications---such as commutative
\code{mmap}s and creating different files in a shared directory---and
reflect findings in previous work~\cite{boyd-wickizer:scaling}.

Finally, to demonstrate the application of the rule and \tool to the
design and implementation of a real system, we use these tests to
guide the implementation of a new research operating system kernel
named \sys.
%
\sys doubles as an existence proof showing that the rule can be
applied fruitfully to the design and implementation of a large
software system, and as an embodiment of novel scalable kernel
implementation techniques.
%
\Cref{sec:sv6} details these techniques and the software designs
necessary to achieve conflict-free commutative operations in \sys's
virtual memory system (\vm) and in-memory file system (\fs).
%
\tool verifies that \sys is conflict-free for
\pyexpr{mscan.ntestcases - mscan.xv6.shared}
(\pyexpr{percent(1-(mscan.xv6.shared/float(mscan.ntestcases)))})
of the tests generated by our POSIX model and addresses many of the
sources of conflicts found in the Linux kernel.
%
\Cref{sec:eval} confirms that \sys's conflict-free implementations of
commutative system calls translate to dramatic improvements in
scalability for both microbenchmarks and application benchmarks on an
80-core x86 machine.

% The paper is organized as follows.  \S\ref{sec:related} relates our thinking
% about scalability to previous work. \S\ref{sec:rule} introduces the
% commutativity rule and argues its correctness. \S\ref{sec:posix} applies the
% rule to an OS interface, identifying opportunities for
% commutativity. \S\ref{sec:tool} describes \tool{}.  \S\ref{sec:model} describes
% \fs and how \tool{} drove its implementation. \S\ref{sec:eval} shows that using
% the commutativity rule results in better scalability on real
% hardware. \S\ref{sec:concl} summarizes our conclusions.

% LocalWords:  rule's RCU microbenchmarks
