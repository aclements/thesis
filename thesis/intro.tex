\section{Introduction}
\label{sec:intro}

\XXX[STATUS]{Restructuring contributions and outline.  Re-organized
  and expanded from SOSP.  Applied: Frans init-176-g5ef5d9a, Eddie
  init-244-ga695479.}

\XXX[AC]{Incorporate clock/heat/cores plot from defense.  Make point
  that core count is growing exponentially (albeit not Moore's
  Law-exponentially).}

This dissertation presents a pragmatic and formal approach to the
design and implementation of scalable multicore software that spans
from the earliest stages of software interface design through
testing and maintenance of complete implementations.

The rest of \thiscref{sec:intro} introduces the multicore
architectures that now dominate general-purpose computing, the
problematic ways in which software developers are coping with these
new architectures, and our interface-driven approach to the design and
implementation of software for multicore architectures.


%\subsection{The rise of multicore architectures}
\subsection{Parallelize or perish}

In the mid-2000s, there was a fundamental shift in the construction of
high performance software.  For decades, CPU clock speeds had ridden
the exponential curve of Moore's Law to greater and greater
computational throughput.  With rising clock speeds, software
naturally got faster.
%
But higher clock speeds require more power and generate more heat, and
around 2005 clock speeds brought CPUs to the thermal dissipation
limits of a few square centimeters of silicon.
%
CPU architects could no longer significantly increase clock speeds, so
they began to increase parallelism by putting more CPUs on the same
chip.
% maintaining a ``coherent'' view of memory and shared access to I/O
% resources.
%
% With the rise of multicore architectures, parallel programming went
% from niche to necessary.
With the widespread adoption of multicore architectures for
general-purpose computing, parallel programming went from niche to
necessary.
%
\emph{Total} cycles per second continues to grow exponentially, but
now software must be increasingly parallel to take advantage of this.
%
However, while software performance naturally scaled with clock speed,
scaling with parallelism is an untamed problem.  Even with careful
engineering, software rarely achieves the holy grail of \emph{linear
  scalability}, where doubling hardware parallelism doubles the
software's performance.

Operating system kernels exemplify both the importance
of parallelism and the difficulty of achieving it.
%
Many applications depend heavily on the shared services and resources
provided by the kernel.
%
As a result, if the kernel doesn't scale, many applications won't
scale.
%
At the same time, the kernel must cope with diverse and unknown
workloads while supporting the combined parallelism of all
applications on a computer.  Additionally, its role as the arbiter of
shared resources make it particularly susceptible to scalability
problems.
%
% \XXX{example?  Any application that creates, opens, read, or writes
%   files is at risk of contending with any other application that uses
%   the file system if the kernel's implementation is insufficiently
%   parallel.}
%
% Underlining the importance of kernel scalability, Linux kernel
% developers have put vast amounts of effort into multicore Linux,
% bringing it to the forefront of putting parallel programming
% techniques into practice.  Lock-free data structures, read-copy-update
% garbage collection, per-core data, and sophisticated synchronization
% mechanisms are all commonplace in the Linux kernel.

Yet, despite the extensive efforts of kernel and application
developers alike, scaling software performance on multicores remains
an inexact science dominated by guesswork, measurement, and expensive
cycles of redesign.
%
The state of the art for evaluating and improving the scalability of
multicore software is to choose some workload, plot performance at
varying numbers of cores, and use tools such as differential
profiling~\cite{mckenney:differential} to identify scalability
bottlenecks.
% \XXX[AC]{Should we cite anything for this?  Is there
%   anything to cite besides our own papers?}  \XXX[AC]{Mention that
%   this is an evolution of sequential optimization techniques?}
%
This focuses developer effort on demonstrable issues, but is
ultimately a near-sighted approach.
%
% Different workloads and higher core counts often reveal new
% bottlenecks, leading to a Sisyphean cycle of performance catch-up and
% leaving workloads and hardware that kernel developers don't have
% access to to fend for themselves.
%
Each new hardware model and workload powers a Sisyphean cycle of
finding and fixing new bottlenecks.
%
Projects such as Linux require continuous infusions of manpower to
maintain their scalability edge.
%
Worse, scalability problems that span layers---for example,
application behavior that triggers kernel bottlenecks---require
cross-layer solutions, and few applications have the reach or
resources to accomplish this.
%
But the deeper problem with this workload-driven approach is that many
scalability problems lie not in the implementation, but in the design
of the
software interface.  By the time developers have an implementation, a
workload, and the hardware to demonstrate a bottleneck,
interface-level solutions may be impractical or impossible.

As an example of interface design that limits implementation
scalability, consider the POSIX \code{open} call~\cite{posix2013}.
%
This call opens a
file by name and returns a \emph{file descriptor}, a number used to
identify the open file in later operations.
%
Even though this identifier is opaque, POSIX requires that \code{open}
return the numerically lowest available file descriptor for the
calling process, forcing the kernel to allocate file descriptors one
at a time, even if many parallel threads are simultaneously opening
files.
%
This simplified the kernel interface during the early days of Unix,
but this interface design choice is now a burden on implementation
scalability.
%
It's an unnecessary burden, too: a simple change to allow \code{open}
to return \emph{any} available file descriptor would enable the kernel
to choose file descriptors scalably.
%
This particular example is well-known~\cite{boyd-wickizer:corey}, but
myriad subtler issues exist in POSIX and other interfaces.

Interface design choices have implications for implementation
scalability.
%
If interface designers could distinguish interfaces that definitely
have a scalable implementation from those that don't, they would have
the predictive power to design \emph{scalable interfaces} that enable
scalable implementations.


\subsection{A rule for interface design}

This dissertation presents a new approach to scalable software design
that starts with the design of scalable software interfaces.
%
This approach makes reasoning about multicore scalability possible
before an implementation exists and before the necessary hardware is
available to measure the implementation's scalability.
%
It can highlight inherent scalability problems, leading to alternate
interface designs.
%
It sets a clear scaling target for the implementation of a
scalable interface.  And it enables systematic testing of an
implementation's scalability.

Reasoning about scalability during interface design requires a notion
of scalability divorced from the details of a specific
implementation and specific hardware.
%
Modern shared-memory multicores have architectural commonalities that
make this possible.  These machines have distributed memory
hierarchies with CPU-local caches and depend on protocols like
MESI~\cite{papamarcos:mesi} to provide a globally consistent,
\emph{coherent} view of memory.  This general architecture enables
general arguments about scalability.
%
In such architectures, a core can scalably read and write data it has
cached exclusively, and scalably read data it has cached in shared
mode. Writing a cache line that was last read or written by another
core is not scalable, however, since the coherence protocol serializes
ownership changes for each cache line, and because the shared
interconnect may serialize unrelated transfers.

We therefore consider a set of operations \emph{scalable} if their
implementations have \emph{conflict-free} memory accesses; that is, no
core writes a cache line that was read or written by another core.
%
When memory accesses are conflict-free, adding more cores will produce
a linear increase in capacity.
%
This is not a perfect model of the complex realities of modern
hardware, but it is a good approximation that we confirm in
\cref{sec:scalability}.

At the core of our approach is this \emph{scalable commutativity
  rule}: In any situation where several operations
\emph{commute}---meaning there's no way to distinguish their execution
order using the interface---they have an implementation whose memory
accesses are conflict-free during those operations.
%
Or, more concisely, \textbf{whenever interface operations commute,
  they can be implemented in a way that scales.}

This rule makes intuitive sense: when operations commute, their
results (return value and effect on system state) are independent of
order.  Hence, communication between commutative operations is
unnecessary, and eliminating it yields conflict-free and scalable
implementations.

The commutativity rule leads to a new way to design scalable
software:
%
analyze the interface's commutativity; if possible, refine or redesign
the interface to improve its commutativity; and then design an
implementation that scales in commutative situations.
%
For example,
consider file creation in a POSIX-like file system. Imagine that
multiple processes create files in the same directory at the same
time. Can the creation system calls be made to scale? Our
first answer was ``obviously not'': the system calls modify the same
directory, so surely the implementation must
serialize access to the directory. But it turns
out these operations commute if the two files have different names
(and no hard or symbolic links are involved) and, therefore, have an
implementation that scales for such names.
One such implementation represents each directory as a hash table
indexed by file name, with an independent lock per bucket,
so that creation of differently named files is conflict-free, barring
hash collisions.
%
Before the rule, we tried to determine if these
operations could scale by analyzing all of the \emph{implementations}
we could think
of.  This process was difficult, unguided, and itself did not scale to
complex interfaces, which
motivated our goal of reasoning about
scalability in terms of interfaces.


\subsection{Contributions}

\XXX{Should more of this stuff go into the rule section and this just
  be a summary?}

The rest of this dissertation presents the scalable commutativity rule
in depth and explores its consequences from interface design to
implementation to testing.

The intuition for the rule presented above is useful in practice, but
not precise enough to reason
about formally.
%
Therefore, the first contribution of this dissertation is a
formalization of the scalable commutativity rule and a proof of the
correctness of this formalized form, which we present in
\cref{sec:rule}.
%
Connecting this formalization to scalability as traditionally measured
requires certain assumptions about multicore hardware, which we verify
experimentally in \cref{sec:scalability}.

An important consequence of this presentation is a novel form of
commutativity we name \emph{\SIM\ commutativity}.
%
The usual definition of commutativity (e.g., for algebraic operations)
is so stringent that it rarely applies to the complex, stateful
interfaces common in systems software.
%
\SIM\ commutativity, in contrast, is \emph{state-dependent} and
\emph{interface-based}, as well as \emph{monotonic}.
%
When operations commute in the context of a specific system state,
specific operation arguments, and specific concurrent operations, we
show that an implementation exists that is conflict-free \emph{for that state
  and those arguments and concurrent operations}.
%
This exposes many more opportunities to apply the rule to real
interfaces---and thus discover scalable implementations---than a more
conventional notion of commutativity would.
%
Despite its logical state dependence, \SIM\ commutativity is
interface-based: rather than requiring all operation orders to produce
identical internal states, it requires the resulting states to be
indistinguishable via the interface.
%
\SIM\ commutativity is thus independent of any specific
implementation, enabling developers to apply the rule directly to
interface design.

In \cref{sec:posix}, this dissertation puts the rule into practice,
contributing a set of general guidelines for designing interfaces that
enable scalable implementations, inspired by both the successes and
failures of POSIX.
%
Using these guidelines, we propose specific modifications to POSIX
that would enable greater scalability.

However, complex interfaces are still complex, and even with the rule
and these interface design guidelines, it can be difficult to spot and
reason about all commutative cases.
%
% Complex interfaces can make it difficult to spot and reason about all
% commutative cases even given the rule.
%
To address this challenge, this dissertation introduces a method to
automate reasoning about interfaces and implementations, embodied in a
software tool named \tool, presented in \cref{sec:tool}.
%
\tool takes an interface model
in the form of a simplified, symbolic implementation, computes precise
conditions under which sets of operations commute, and tests an
implementation for conflict-freedom under these conditions.  This tool can be
integrated into the software development process to drive initial design and
implementation, to incrementally improve existing implementations, and to
help developers understand the commutativity of an interface.

In \cref{sec:topic:model}, we apply \tool to a model of
\pyexpr{len(mscan.calls)} POSIX file system and virtual memory system
calls.
%
From this model,
\tool generates \pyexpr{mscan.ntestcases} tests of commutative
system call pairs, all of which can be made conflict-free
according to the rule.
%
Applying this suite to Linux, we find that the Linux kernel is
conflict-free for \pyexpr{mscan.ntestcases - mscan.linux.shared}
(\pyexpr{percent(1-(mscan.linux.shared/float(mscan.ntestcases)))}) of
these cases.
%
Many of the commutative cases where Linux is not conflict-free are
important to applications---such as commutative
\code{mmap}s and creating different files in a shared directory---and
reflect bottlenecks found in previous
work~\cite{boyd-wickizer:scaling}.

Finally, to demonstrate the application of the rule and \tool to the
design and implementation of a real system, we use these tests to
guide the implementation of a new research operating system kernel
named \sys.
%
\sys doubles as an existence proof showing that the rule can be
applied fruitfully to the design and implementation of a large
software system, and as an embodiment of novel scalable kernel
implementation techniques.
%
\Cref{sec:sv6} details these techniques and the software designs
necessary to achieve conflict-free commutative operations in \sys's
virtual memory system (\vm) and in-memory file system (\fs).
%
\tool verifies that \sys is conflict-free for
\pyexpr{mscan.ntestcases - mscan.xv6.shared}
(\pyexpr{percent(1-(mscan.xv6.shared/float(mscan.ntestcases)))})
of the tests generated by our POSIX model and addresses many of the
sources of conflicts found in the Linux kernel.
%
\Cref{sec:eval} confirms that \sys's conflict-free implementations of
commutative system calls translate to dramatic improvements in
measured scalability for both microbenchmarks and application
benchmarks on an 80-core x86 machine.


\XXX![AC]{New intro stuff below here.}


\subsection{Contributions}

This dissertation's broad contribution is a new approach to building
scalable software using interface-based reasoning to guide design,
implementation, and testing.  This dissertation has the following
intellectual contributions:

\begin{itemize}

\item The scalable commutativity rule, its formalization, and a proof
  of its correctness.

\item \SIM commutativity, a novel form of interface commutativity that
  is state-sensitive and interface-based.  As we demonstrate with
  \tool, \SIM commutativity enables us to identify myriad commutative
  cases in the highly stateful POSIX interface.

\item A set of guidelines for commutative interface design based on
  \SIM commutativity.  Using these guidelines, we propose specific
  enhancements to POSIX and empirically demonstrate that these changes
  enable dramatic improvements in application scalability.

\item An automated method for reasoning about interface commutativity
  and generating implementation scalability tests using symbolic
  execution.  This method is embodied in a new tool named \tool, which
  generates \pyexpr{mscan.ntestcases} tests of commutative operations
  in our model of \pyexpr{len(mscan.calls)} POSIX file system and
  virtual memory system operations.  These tests cover many subtle
  cases, identify many substantial scalability bottlenecks in the
  Linux kernel, and guide the implementation of \sys.

\end{itemize}

This dissertation also contributes several implementation techniques,
embodied in a new operating system named \sys:

\begin{itemize}

\item \refcache, a novel scalable reference counting scheme that
  achieves conflict-free \emph{increment} and \emph{decrement}
  operations with efficient periodic reconciliation and $O(1)$
  per-object space overhead.

\item \vm, a POSIX virtual memory system based on radix trees that is
  conflict-free for commutative \code{mmap}, \code{munmap}, and
  \code{pagefault} operations.

\item \fs, a POSIX file system that is conflict-free for the vast
  majority of commutative operations.

\end{itemize}

We validate \vm and \fs, and thus their design methodology based on
the rule and \tool, by evaluating their performance and scalability on
an 80-core x86 machine.


\subsection{Outline}

The remainder of this dissertation begins by relating our thinking
about scalability to previous work.

We then turn to formalizing and proving the scalable commutativity
rule, which we approach in two steps.  First, \cref{sec:rule}
formalizes and proves that commutative operations have conflict-free
implementations, developing and justifying \SIM commutativity in the
process.  Second, \cref{sec:scalability} empirically demonstrates that
conflict-free operations are scalable on modern, large multicore
machines.

We next turn to applying the rule.  \Cref{sec:posix} starts by
applying the rule to interface design, developing a set of guidelines
for designing interfaces that enable scalable implementations and
proposing specific modifications to POSIX that broaden its
commutativity.

\Cref{sec:tool} presents \tool, which applies the rule to automate
reasoning about interface commutativity and the conflict-freedom of
implementations.  \Cref{sec:linux} uses \tool to analyze the Linux
kernel and show that the rule and \tool can systematically pinpoint
significant scalability problems even in mature systems.

Finally, we turn to the implementation of scalable systems guided by
the rule.  \Cref{sec:sv6} describes the implementation of \sys and how
it achieves conflict-freedom for the vast majority of commutative
POSIX file system and virtual memory operations.  \Cref{sec:eval}
confirms that theory translates into practice by evaluating the
performance and scalability of \sys on real hardware for several
microbenchmarks and application benchmarks.

\XXX[AC]{\Cref{sec:concl}}

% The paper is organized as follows.  \S\ref{sec:related} relates our thinking
% about scalability to previous work. \S\ref{sec:rule} introduces the
% commutativity rule and argues its correctness. \S\ref{sec:posix} applies the
% rule to an OS interface, identifying opportunities for
% commutativity. \S\ref{sec:tool} describes \tool{}.  \S\ref{sec:model} describes
% \fs and how \tool{} drove its implementation. \S\ref{sec:eval} shows that using
% the commutativity rule results in better scalability on real
% hardware. \S\ref{sec:concl} summarizes our conclusions.

% LocalWords:  rule's RCU microbenchmarks
