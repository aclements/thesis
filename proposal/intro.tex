\section{Introduction}
\label{sec:intro}

In computation-heavy domains, such as scientific computing and
graphics, applications can use multi-core hardware well on their own.
%
But the situation is different for systems applications, by which we
mean programs that make heavy use of other \emph{interfaces} (provided
by the operating system kernel, infrastructure libraries, databases, and
the like).
%
Such applications often have scalability problems caused not
by their own code but by the interfaces they use.
%
This makes a systems application's scalability difficult to analyze in
isolation.
%
For instance, multi-threaded applications
in which each thread allocates memory
might run into a scalability bottleneck due to the single lock that protects
a process's address space in Linux~\cite{clements:bonsai}, even though there
is no fundamental reason why these operations should not scale.
%
But the application programmer had no way to predict this bottleneck.
%
The scalability limit doesn't appear in the manual, and isn't obvious
from the system interface.
%
Unknown scalability consequences cause problems for kernel developers, too,
who are left unsure which interfaces need to scale, and which need
not---or cannot.
%
Similar limitations and concerns hold for interfaces ranging from
databases to system calls to interfaces within an OS kernel.

The result is a reactive approach to designing for scalability.
%
Systems programs aren't made to scale until bottlenecks become
painfully obvious.
%
For instance, kernel developers watch for applications
whose performance does not scale well on existing machines due to kernel
bottlenecks, and then modify the kernel to improve the scalability of the
relevant services~\cite{cacm-real-world}.  These modifications typically
involve changing data structures to use finer-grained locks, applying
RCU~\cite{rcu:linux} to avoid read locks altogether, and partitioning or
replicating data per core.
%
Though operating systems like Linux scale well for many important applications on current
hardware~\cite{boyd-wickizer:scaling},
%
the reactive approach has serious disadvantages.
%
Any application might uncover a new kernel scalability bottleneck,
but only those bottlenecks encountered by ``important'' applications
are addressed quickly.
%
Application developers must modify their code to avoid bottlenecks in
the kernel and in other services, such as databases, but these
bottlenecks often aren't documented or may not even be known.
%
Effort is duplicated as multiple programmers discover similar
workarounds, and then must modify them as underlying problems are
resolved and the workarounds become bottlenecks themselves.

Systems programs will be fragile---scalable only on today's
hardware and sensitive to changes in other software layers---until
systems developers have better tools for \emph{modular scalability
  reasoning}.
%
Systems programmers should be able to reason about scalability
without understanding the \emph{implementations} of
the systems with which their programs interact.
%
Documented \emph{interface-level} properties should, as far as
possible, determine the scalability of a program.

This proposal develops tools for modular scalability reasoning that we
call the \textbf{commutativity rule}.\footnote{This thesis proposal is
  in collaboration with faculty members Frans Kaashoek, Nickolai
  Zeldovich, Robert Morris, and Eddie Kohler.  To recognize their
  help, this proposal uses the words ``we,'' ``authors,'' etc.}
\XXX[Scalable commutativity rule?  External commutativity rule?]
%
The main idea is intuitive (our proposed work will make it precise).
%
Two operations \emph{commute} if their order of execution does not
affect their results and the results of
later operations cannot distinguish which operation executed first.
%
Commutativity is a property of an interface specification
and is independent of its
implementation.  A particular implementation of an interface
\emph{scales} when concurrent operations do not affect each other's
execution times; this is ``perfect'' scalability in which
additional cores produce linear increases in throughput.
The commutativity rule observes that, even though
scalability is an implementation property, an interface has a certain
inherent scalability that an implementation may exploit.
Hence, programmers can use the rule to evaluate scalability
independent of implementations; not only is this far easier than
reasoning about code, but it is possible before an implementation even
exists.
%
Specifically, \emph{whenever two
operations commute, there exist implementations of those operations that
scale}.
%
More broadly stated, we believe that \emph{the more commutative} an
interface is, \emph{the more scalable} its implementation can be.

This has two important consequences, one for interface design and use
and one for implementation.

First, the commutativity rule suggests that an operation that
\emph{never} commutes will be difficult or impossible to scale.
%
A system designer need not try to scale such an operation, since the
attempt will likely fail.
%
Instead, the designer should try to create an alternate interface that
\emph{does} commute.
%
Similarly, an interface \emph{user} should avoid non-commutative
interfaces in favor of commutative ones.
%
For example, two \syscall{open} system calls by different threads of
the same POSIX process are not commutative, because each such call
must return the lowest unused file descriptor.
%
A more scalable \syscall{open} interface would return \emph{any}
unused file descriptor.

Second, the commutativity rule suggests that operations that
\emph{do} commute \emph{can be} made scalable.
%
This holds even for operations that commute only in selected states or for
selected arguments and, as we show, many interfaces fall between
universal commutativity and universal non-commutativity.
%
For example, consider the \syscall{mmap} system call for mapping
memory.
%
If two threads of the same process call \syscall{mmap(x)} at the same
address \syscall{x}, then clearly the calls do not commute: only one
of the memory map requests can win.
%
But more commonly, different threads will call \syscall{mmap} at
\emph{different} addresses (for example, to allocate memory within
thread-specific regions of virtual memory).
%
Since such calls commute, they can also scale~\cite{clements:bonsai},
even if they do not scale in current implementations of
\syscall{mmap}.

The commutativity rule lets systems implementers, interface
designers, and application developers think of scalability in terms of
interfaces.
%
This complements the reactive approach to scalable software design, and
addresses many of its problems, by providing a clear statement of
interface scalability that does not require reasoning about
potential implementations.
%
Scalability problems aren't automatically solved;
%
scalable implementations of commutative interfaces still require effort.
%
But commutativity rules perform two critical functions
lacking from our current understanding of scalability: they highlight
when implementations should strive to be scalable, and which interfaces
are inherently unscalable.

An example will show why commutativity rules can help even experienced
programmers understand the inherent scalability of an interface.
%
Consider the \syscall{open} system call, as used to create a file in a
directory.
%
Further imagine that two \syscall{open} calls from different processes
create two files in the same directory at the same time.
%
Can those system calls be made to scale?
%
Our first answer was ``obviously not.''
%
The two system calls modify the same shared state, namely the containing
directory.
%
Surely in any implementation the two calls would contend on this shared
state, either via a lock or via non-blocking synchronization on one or
more shared cache lines. (Both forms of contention limit scalability.)
%
But commutativity shows this isn't really true.
%
If the two system calls create files \emph{with different names}, then the
success or failure of each system call is independent; later system calls can't
distinguish which completed first; and the operations commute.
%
Our commutativity rules say, therefore, that there must exist some
implementation of \syscall{open} that scales for these calls.
%
In hindsight, this implementation is straightforward.
%
If the directory is implemented as a hash table, and each hash bucket has an
independent lock, the two file creations can proceed (most likely) without
communicating.
%
The authors initially believed that this example could not scale; our
mistake was to analyze only the implementations we could think of for
scalability. It was examples like this that motivated our search for
the rule.
%
And file creation is not the only example: commutativity rules have pointed us
to similar insights in other interfaces, including the virtual memory system and
\syscall{mmap}.

\paragraph{Research challenges}

The proposed research addresses new problems in specification and
implementation.  Our overall research plan is to explore and formalize
the commutativity rule and modular scalability reasoning.  A precise
definition of commutativity must depend on interface properties,
system state, and potentially even future operations, while remaining
intuitive.  We will build the {\sys} operating system for hands-on
experience with using these ideas to design scalable interfaces and
construct scalable implementations.

More specifically, we plan to work on the following (see
\S\ref{sec:research}).
(1) Formalizing definitions of scalability that match various
hardware designs, and understanding the situations where real
hardware scales better or worse than our abstract model.
(2) Making the definition of commutativity more precise,
and proving that it implies scalability.
(3) Exploring intermediate degrees of commutativity.
(4) Exploring ways in which commutativity analyses can help
programmers find scalable implementations.
(5) Exploring notions of amortized scalability, which allow
small amounts of inter-core interaction, and corresponding
commutativity-like rules.
(6) Finding ways to change POSIX interfaces to allow better
scalability.
(7) Developing checking tools to help programmers verify that
their code achieves the intended scalability.
(8) Defining documentation conventions that allow application
programmers to predict whether their use of interfaces will
be scalable.
(9) Applying the rule and using scalable system calls in a
high-performance application.

\paragraph{Expected contributions}

The long-term impact of this research is ideas about how to structure
software so that it is scalable by design, by thinking about interface
commutativity.  Our ideas about commutative interface design for
scalability apply equally well to systems other than operating
systems, and in the long term we hope that programmers find our ideas
useful to think about how to design interfaces and structure software
to obtain good scalability.

In the near term, we hope to contribute new techniques for how to make
operating systems more scalable.  Our goal is that these techniques
address problems that widely used operating systems experience, with
the hope that our solutions will influence those operating systems.
Because of our work with
Linux~\cite{boyd-wickizer:scaling,clements:bonsai}, we have developed
a good understanding of the scalability challenges faced by real
operating systems.  Our \sys operating system will demonstrate the
application of modular scalability reasoning through the commutativity
rule to such challenges and act as a proving ground for our
techniques.
